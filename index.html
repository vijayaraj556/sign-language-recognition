<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sign Language Recognition</title>
    <!-- Tailwind CSS CDN for easy styling and responsiveness -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f4f8; /* Light blue-gray background */
        }
        /* Custom styles for the video and canvas to ensure proper display */
        #videoFeed, #canvasOutput {
            width: 100%;
            max-width: 640px; /* Max width for video/canvas */
            height: auto;
            border-radius: 1rem; /* Rounded corners */
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); /* Subtle shadow */
            background-color: #000; /* Black background for video area */
        }
        /* Hide canvas by default, only show if needed for drawing */
        #canvasOutput {
            display: none;
            border: 2px solid #3b82f6; /* Blue border for canvas */
        }
    </style>
</head>
<body class="flex flex-col items-center justify-center min-h-screen p-4">

    <div class="bg-white p-8 rounded-2xl shadow-xl max-w-2xl w-full text-center">
        <h1 class="text-4xl font-bold text-gray-800 mb-6">Sign Language Recognition</h1>

        <!-- Video feed from webcam -->
        <div class="mb-6 flex justify-center">
            <video id="videoFeed" autoplay playsinline class="rounded-xl"></video>
            <!-- Canvas to potentially draw processed frames or hand landmarks (initially hidden) -->
            <canvas id="canvasOutput" class="rounded-xl"></canvas>
        </div>

        <!-- Control buttons -->
        <div class="flex flex-col sm:flex-row justify-center space-y-4 sm:space-y-0 sm:space-x-4 mb-6">
            <button id="startButton"
                    class="bg-blue-600 hover:bg-blue-700 text-white font-semibold py-3 px-6 rounded-lg
                           shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                Start Recognition
            </button>
            <button id="stopButton"
                    class="bg-red-500 hover:bg-red-600 text-white font-semibold py-3 px-6 rounded-lg
                           shadow-md transition duration-300 ease-in-out transform hover:scale-105"
                    disabled>
                Stop Recognition
            </button>
        </div>

        <!-- Display area for recognized signs -->
        <div class="bg-gray-100 p-6 rounded-xl border border-gray-200">
            <h2 class="text-2xl font-semibold text-gray-700 mb-3">Recognized Sign:</h2>
            <p id="recognizedSign" class="text-3xl font-bold text-green-700">
                Waiting for input...
            </p>
            <p id="statusMessage" class="text-sm text-gray-500 mt-2">
                Click 'Start Recognition' to begin.
            </p>
        </div>

        <!-- Message box for alerts (instead of alert()) -->
        <div id="messageBox" class="hidden fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4 z-50">
            <div class="bg-white p-6 rounded-lg shadow-xl text-center max-w-sm w-full">
                <p id="messageBoxText" class="text-lg font-medium text-gray-800 mb-4"></p>
                <button id="messageBoxClose" class="bg-blue-500 hover:bg-blue-600 text-white py-2 px-4 rounded-md">
                    OK
                </button>
            </div>
        </div>
    </div>

    <script>
        // Get references to HTML elements
        const videoFeed = document.getElementById('videoFeed');
        const canvasOutput = document.getElementById('canvasOutput');
        const recognizedSign = document.getElementById('recognizedSign');
        const statusMessage = document.getElementById('statusMessage');
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const messageBox = document.getElementById('messageBox');
        const messageBoxText = document.getElementById('messageBoxText');
        const messageBoxClose = document.getElementById('messageBoxClose');

        let stream = null; // To hold the webcam stream
        let recognitionInterval = null; // To hold the interval for recognition loop

        // Function to show a custom message box
        function showMessageBox(message) {
            messageBoxText.textContent = message;
            messageBox.classList.remove('hidden');
        }

        // Event listener for message box close button
        messageBoxClose.addEventListener('click', () => {
            messageBox.classList.add('hidden');
        });

        // Function to start webcam and recognition
        async function startRecognition() {
            try {
                // Request access to the user's webcam
                stream = await navigator.mediaDevices.getUserMedia({ video: true });
                videoFeed.srcObject = stream;
                statusMessage.textContent = 'Webcam active. Analyzing hand gestures...';
                recognizedSign.textContent = 'Detecting...';

                // Enable/disable buttons
                startButton.disabled = true;
                stopButton.disabled = false;

                // Start the recognition loop after the video metadata is loaded
                videoFeed.onloadedmetadata = () => {
                    videoFeed.play();
                    // Set canvas dimensions to match video
                    canvasOutput.width = videoFeed.videoWidth;
                    canvasOutput.height = videoFeed.videoHeight;
                    // Start the recognition process
                    recognitionInterval = setInterval(performRecognition, 100); // Run every 100ms
                };

            } catch (err) {
                console.error('Error accessing webcam:', err);
                statusMessage.textContent = 'Error: Could not access webcam. Please ensure camera permissions are granted.';
                recognizedSign.textContent = 'Error!';
                showMessageBox('Could not access your webcam. Please check camera permissions and try again.');
                // Ensure buttons are in correct state on error
                startButton.disabled = false;
                stopButton.disabled = true;
            }
        }

        // Function to stop webcam and recognition
        function stopRecognition() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop()); // Stop all tracks (video, audio)
                videoFeed.srcObject = null;
                stream = null;
            }
            if (recognitionInterval) {
                clearInterval(recognitionInterval); // Stop the recognition loop
                recognitionInterval = null;
            }
            statusMessage.textContent = 'Recognition stopped. Click "Start Recognition" to resume.';
            recognizedSign.textContent = 'Waiting for input...';

            // Enable/disable buttons
            startButton.disabled = false;
            stopButton.disabled = true;
        }

        // Placeholder function for actual sign language recognition
        // In a real application, you would integrate a machine learning model here (e.g., TensorFlow.js)
        function performRecognition() {
            // This is where you would typically:
            // 1. Get the current frame from the videoFeed.
            //    const context = canvasOutput.getContext('2d');
            //    context.drawImage(videoFeed, 0, 0, canvasOutput.width, canvasOutput.height);
            //    const imageData = context.getImageData(0, 0, canvasOutput.width, canvasOutput.height);

            // 2. Pass the image data to a pre-trained machine learning model.
            //    Example (conceptual, requires TensorFlow.js or similar setup):
            //    const predictions = await model.predict(imageData);
            //    const recognizedGesture = processPredictions(predictions);

            // For this example, we'll just simulate recognition based on whether the video is playing.
            if (videoFeed.readyState === videoFeed.HAVE_ENOUGH_DATA) {
                // Simulate a recognized sign
                const signs = ["Hello", "Thank You", "Yes", "No", "Love", "Please"];
                const randomSign = signs[Math.floor(Math.random() * signs.length)];
                recognizedSign.textContent = randomSign;
            } else {
                recognizedSign.textContent = 'Processing...';
            }
        }

        // Event listeners for buttons
        startButton.addEventListener('click', startRecognition);
        stopButton.addEventListener('click', stopRecognition);

        // Initial state: ensure stop button is disabled
        stopButton.disabled = true;
    </script>
</body>
</html>
